from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
from feedgenerator import Rss201rev2Feed
from datetime import datetime
import asyncio
import json
import re

# è¤‡æ•°ã®URLã‚’ãƒªã‚¹ãƒˆã§ç”¨æ„
urls = [
    'https://ledge.ai/categories/business/',
    'https://ledge.ai/categories/learning/', 
    'https://ledge.ai/categories/engineering/',
    'https://ledge.ai/categories/academic/',
    'https://ledge.ai/categories/public/',
    'https://ledge.ai/categories/entertainment/'
]

async def main():
    print("RSSãƒ•ã‚£ãƒ¼ãƒ‰ç”Ÿæˆã‚’é–‹å§‹ğŸš€")

    # RSSãƒ•ã‚£ãƒ¼ãƒ‰ã®åˆæœŸåŒ–
    feed = Rss201rev2Feed(
        title="Ledge.ai è¤‡æ•°ã‚«ãƒ†ã‚´ãƒª",
        link="https://ledge.ai",
        description="Ledge.aiã®è¤‡æ•°ã‚«ãƒ†ã‚´ãƒªã®æœ€æ–°è¨˜äº‹",
        language="ja",
        pretty=True
    )

    async with async_playwright() as p:
        browser = await p.chromium.launch()  # chromium ã‚’ä½¿ç”¨
        page = await browser.new_page()

        for getURL in urls:
            print(f"{getURL} ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã‚ˆğŸŒ")
            try:
                print(f"{getURL} ãƒšãƒ¼ã‚¸ã«ç§»å‹•ä¸­...")
                await page.goto(getURL, timeout=30000, wait_until='load')
                print("ãƒšãƒ¼ã‚¸ã«ç§»å‹•ã—ãŸâœˆï¸")

                # ãƒšãƒ¼ã‚¸ã®HTMLã‚’å–å¾—
                print("HTMLå–å¾—ä¸­...")
                html = await page.content()

                # BeautifulSoupã§è§£æ
                print("HTMLè§£æä¸­...")
                soup = BeautifulSoup(html, 'html.parser')

                # window.__NUXT__ã®å†…å®¹ã‚’å–å¾—ã—ã¦JSONãƒ‡ãƒ¼ã‚¿ã‚’Pythonã®è¾æ›¸ã«å¤‰æ›
                print("JSONãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­...")
                nuxt_data = json.loads(await page.evaluate('() => JSON.stringify(window.__NUXT__)'))
                print(f"JSONãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆä¸€éƒ¨ï¼‰ï¼š{str(nuxt_data)[:100]}... ğŸ“¥")

                # "data"ã‚­ãƒ¼ã®ä¸­ã«ã‚ã‚‹"articles"ã‚­ãƒ¼ã®"data"ã‚­ãƒ¼ã®å€¤ã‚’å–å¾—
                print("è¨˜äº‹ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­...")
                try:
                    articles = nuxt_data["data"][f"/categories/{getURL.split('/')[-2]}"]["articles"]["data"]
                except KeyError as e:
                    print(f"ã‚¨ãƒ©ãƒ¼: JSONãƒ‡ãƒ¼ã‚¿ã«ã‚­ãƒ¼ {e} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚JSONãƒ‡ãƒ¼ã‚¿ã®æ§‹é€ ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                    print(f"getURL: {getURL}")
                    print(f"nuxt_data['data'].keys(): {nuxt_data['data'].keys()}")
                    articles = []  # articles ã‚’ç©ºãƒªã‚¹ãƒˆã«è¨­å®š

                if not articles:
                    print("è­¦å‘Š: è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ãŒç©ºã‚„ã§â—ï¸")
                else:
                    print(f"å–å¾—ã—ãŸè¨˜äº‹æ•°ï¼š{len(articles)} ğŸ“š")

                # 12å€‹ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                print("è¨˜äº‹ãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸­...")
                for article in articles[:12]:
                    title = article['attributes']['title']
                    date_str = article['attributes']['createdAt']
                    date_obj = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
                    url = "https://ledge.ai/articles/" + article['attributes']['slug']
                    description = re.sub(r'\[.*?\]\(.*?\)', '', article['attributes']['contents'][0]['content'])

                    # XMLã®ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—å‡¦ç†
                    title = title.replace('&', '&').replace('<', '<').replace('>', '>').replace('"', '"').replace("'", ''')
                    description = description.replace('&', '&').replace('<', '<').replace('>', '>').replace('"', '"').replace("'", ''')

                    # ã‚¢ã‚¤ãƒ†ãƒ ã‚’ãƒ•ã‚£ãƒ¼ãƒ‰ã«è¿½åŠ 
                    feed.add_item(
                        title=title,
                        link=url,
                        description=description,
                        pubdate=date_obj
                    )
                print("RSSãƒ•ã‚£ãƒ¼ãƒ‰ã«ãƒ‡ãƒ¼ã‚¿è¿½åŠ ã—ãŸğŸ“")

            except Exception as e:
                print(f"ã‚¨ãƒ©ãƒ¼: {e}")
                print(f"URL: {getURL} ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
                import traceback
                traceback.print_exc()  # ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’å‡ºåŠ›

        await browser.close()

    # RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã—
    print("RSSãƒ•ã‚£ãƒ¼ãƒ‰æ›¸ãå‡ºã—ä¸­...")
    with open('feed.xml', 'w') as f:
        feed.write(f, 'utf-8')

    print("è¤‡æ•°ã‚«ãƒ†ã‚´ãƒªã®RSSãƒ•ã‚£ãƒ¼ãƒ‰ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸğŸ‰")

# éåŒæœŸé–¢æ•°ã‚’å®Ÿè¡Œ
asyncio.run(main())
