<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>Ledge.aiビジネスカテゴリ</title><link>https://ledge.ai/categories/business/</link><description>Ledge.aiのビジネスカテゴリの最新記事</description><language>ja</language><lastBuildDate>Tue, 05 Sep 2023 07:25:06 +0000</lastBuildDate><item><title>OpenAIついにEUデータ保護規則違反で訴訟に「事前に当局と協議すべきだった」</title><link>https://ledge.ai/articles/openai_sued_for_gdpr_violation</link><description>:::small
photo by {target=“_blank”}
:::

ポーランドの法律事務所GP Partnersは2023年8月31日、サイバーセキュリティとプライバシーの研究者 Lukasz Olejnik博士の原告代理人として、米国OpenAIが欧州連合（EU）の包括的な一般データ保護規則（GDPR）に違反したとして、ポーランドのデータ保護当局に訴状を提出したと{target=“_blank”}した。

EUはGDPRを2016年4月に制定、2018年5月25日に施行している。EUのデータ保護指令より個人データやプライバシーの保護に関して厳格に規定した内容だ。

{target=“_blank”}によると、訴状では、OpenAIが法的根拠、透明性、公平性、データアクセス権、設計上のプライバシーといった幅広い側面にわたりGDPRに違反していると主張している。経緯としては、Olejnik博士がOpenAIが開発する対話型AI（チャットボット）「ChatGPT」を使用して自分の伝記を生成したところ、いくつかの誤謬（ごびゅう）を含んでいるのに気づき、3月末頃にOpenAIに連絡して修正を求めた。また博士はGDPRに基づき、OpenAIが持つ彼の情報を提供することも促した。6月にかけて博士とOpenAIの間でメールのやりとりがあり、OpenAIは一部の情報を提供したが、法が義務付ける内容としては不十分だったという。

GP Partnersは、ChatGPTのようなAIの開発と「データ主体（つまりOlejnik博士）」の権利を調和させることは可能なはずだとしている。またAIの開発に数十億ドルを費やすのであれば一部をGDPR準拠のために割り当てることはできるし、もしOpenAIが自社サービスとGDPRの調和に疑問を抱いた場合、制度にのっとって事前にデータ保護当局と協議しておくべきだったとも指摘した。

!
:::small
{target=“_blank”}
:::


:::box

:::
:::box

:::
:::box

:::</description><pubDate>Tue, 05 Sep 2023 07:25:06 +0000</pubDate></item><item><title>スタバ 中国・深センにDX施設、3年間で300億円を投資 データ分析やモデリングの拠点に</title><link>https://ledge.ai/articles/starbucks_sitc</link><description>:::small
Unsplashのkevsが撮影した写真
:::

米国スターバックスは2023年8月17日、中国の広東省深セン市に「Starbucks China Innovation and Technology Center（SITC）」を設立すると{target=“_blank”}した。2023年9月に稼働開始する予定で、深セン市などによると今後3年間で約15億元（約300億円）を投資する。

SITCはデータ分析、モデリングなどを駆使して、中国での小売業務や、店舗やWebサイトなどを連携させたオムニチャネル事業のデジタル化を進める。

店舗運営の簡素化と自動化、革新的なコーヒーなどの飲料や食品の提供、サプライチェーンや店舗開発、運営のデジタル化、環境フットプリントの削減に焦点を当てる。スターバックスは2023年現在、世界86市場で37,000以上の店舗を運営。2025年までに、中国に9,000店舗を展開する。2023年秋には、統合配送センターなどのインフラを備える「Starbucks China Coffee Innovation Park」のオープンも予定している。

:::box

:::
:::box

:::


</description><pubDate>Mon, 04 Sep 2023 05:56:22 +0000</pubDate></item><item><title>GoogleがAIを攻撃する6つの手法公開 個人情報の抽出からAIそのものを盗み出すやり方まで</title><link>https://ledge.ai/articles/google_ai_red_team</link><description>:::small
画像の出典：{target=“_blank”}
:::
米国Googleは、AIへの攻撃を専門にする組織「{target=“_blank”}」を運用している。AIの弱点を見つけ出し、対策を立てるためだ。2023年7月19日には、同じような組織を作る他社の参考となるよう報告書を{target=“_blank”}した。

セキュリティの世界では、システムの防御役を担う「ブルーチーム」と別に攻撃役を担う「レッドチーム」を作ることで、弱点を見つけ出し対策をとる手法が広まっている。

今回の報告書を公表したDaniel Fabian氏は、10年前に設立したGoogleのレッドチーム責任者。国家や有名な高度持続的脅威（APT） グループからハクティビスト（Hacktivist：政治的、社会的な主張・目的のためにハッキングを行う個人や集団）、個人の犯罪者、さらには悪意のある内部関係者に至るまで、さまざまな敵を模倣し、Googleのシステムに攻撃をしかける役割を果たす。

報告書ではAI専門の新たなレッドチームを取り上げ、TTP「Tactics：戦術」「Techniques:技術」「Procedures:手順」について述べた。概要は次の通り。

**プロンプト攻撃（Prompt attacks）**
大規模言語モデル（LLM）に対するテキストの指示「プロンプト」の文言を工夫して的確な動作をさせる「プロンプトエンジニアリング」を悪用する手法で、例えばLLMを採用したフィッシング対策や文法校正などのアプリケーションに開発者が意図しない動作をせられる。

**学習データの抽出（Training data extraction）**
AIに学習データが含んでいた個人情報（PII）やパスワードなどを再現させ引き出す手法。例えばLLMはインターネット上の膨大なデータから学習するため、事前にほとんどのPIIを削除しても、一部は処理が漏れている場合がある。あるいは電子メール作成時に内容を自動補完するようなAIの開発者が、学習データに「差分プライバシー」などの適切な処理をしていない場合などにも効果を発揮する。

**モデルのバックドア（Backdooring the model）**
さまざまなAIのモデルに「トリガーワード（引き金となる単語）」などの裏口（バックドア）になる情報をあらかじめ仕込んでおき、モデルが稼働したあと、単語を入力するなどして望み通りの動作をさせる。今やインターネットなどで多くのモデルが公開中だが、研究者がそれらをダウンロードして利用した場合に、予期せぬふるまいをさせられる。

**敵対的サンプル（Adversarial examples）**
画像や音声などに人間には認識できない微小な変更を加えておき、AIが処理した際だけ予期せぬ動作を引き起こす手法。例えば人間には猫のように見えるがAIは犬と認識してしまう画像や、人間には特定の文章を読み上げているように聞こえるがAIが文字起こしするとまったく別の文章になる音声などだ。

**データ汚染（Data poisoning）**
AIの学習データに悪意ある内容を混ぜ込むことで影響を与える手法。AIの開発者はしばしばインターネット上のデータをWebスクレイピングというやり方で抽出したり、コーパスと呼ぶ公開のデータベースを参照したりしてAIの学習に使用するため、攻撃者は汚染データを挿入する機会がある。

**流出（Exfiltration）**
AIのモデルそのものを盗み出したり、特徴を模倣したりする手法。AIの機能を外部から利用できるアプリケーション・プログラミング・インターフェイス（API）の仕組みを悪用して入出力を分析し、よく似たモデルを作り上げる、あるいはもっと一般に知られたやり方としてモデルの開発者にフィッシング詐欺をしかけるといったものがある。

いずれもどのように攻撃が起きるか具体的に描写した複数の仮想シナリオを添えて解説している。これらの手法は、従来のレッドチームの手法に加えて実践してみる必要があるという。

なお、従来のレッドチームも出発点としては悪くないが、AIへの攻撃は急速に複雑になってゆくため、可能であればセキュリティとAIの両方の専門家と協力できる組織を作ることが望ましい。しかしAIレッドチームは従来のレッドチームを置き換える訳ではなく、連携して活動する機会も多い。

ちなみに今回挙げた攻撃手法の多くは標的となるシステムや機械学習モデルをロックダウン（閉鎖）するなど従来のセキュリティ管理でリスクを大幅に低減できるという。攻撃の検出についても、多くは従来の方法で可能。ただしプロンプト攻撃などに対しては複数のセキュリティモデルを階層化するなどの対応が必要になる。

:::box

:::
:::box

:::
:::box

:::</description><pubDate>Mon, 04 Sep 2023 09:54:19 +0000</pubDate></item><item><title>生成AIがCOBOLをJavaへ高速変換 IBMが「watsonx Code Assistant for Z」公開</title><link>https://ledge.ai/articles/watsonx_code_assistant_for_z</link><description>生成AIを使用し、データ処理用の古いプログラミング言語COBOLで作成したアプリケーションをより新しい言語Javaへ高速で変換するサービス「{target=“_blank”}」を米国IBMが現地時間2023年8月22日付けで発表した。9月11～13日に米国ラスベガスで開催するイベント「IBM TechXchange」でプレビューを公開し、10～12月には同社のメインフレーム「IBM Z」シリーズ向けに提供開始予定。

@

115種類のプログラミング言語に対応するIBMの生成AI「watsonx.aiコード・モデル」を活かしたもの。1兆5,000億のトークン（語彙）から学習し、200億パラメーター（変数）のデータサイズを持つ。コード自動化用の生成AI基盤モデルとしては最大級のひとつとしている。

COBOLで構築したアプリのコードを選択的、段階的にJavaに変換できる。対象となる潜在COBOLコードは数十億行に上るとIBMは推定している。また分析ツール「Application Discovery and Delivery Intelligence（ADDI）」と組み合わせて展開し、アプリのリファクタリング（再設計）、自動テスト、結果の検証なども行える。アプリの改修に伴う総コスト、複雑性、リスクの管理なども可能としている。</description><pubDate>Sat, 02 Sep 2023 06:41:20 +0000</pubDate></item><item><title>生成AIフェイク防げ “見えない電子透かし” 画像に入れる「SynthID」登場</title><link>https://ledge.ai/articles/synthid</link><description>AIが生成した画像に「電子透かし（watermark）」を入れて識別するツール「{target=“_blank”}」のベータ版を米国Google傘下の英国Google DeepMindが開発したと現地時間の2023年8月29日付けで発表した。画像を構成する微小な点、ピクセル（画素）の間に人間の目には感知できない情報を直接埋め込む。Google Cloudの機械学習基盤「Vertex AI」で画像生成AI「Imagen」を利用している一部の企業が導入できる。

@

生成AIは本物そっくりの偽写真などを作り出せるとして米国政府なども問題視し、ほかと区別できる仕組みを求める声が高まっている。

対策のひとつが電子透かし。ただ画像の端などにスタンプのように目に見えるかたちで入れる従来の手法だと、切り抜きなどの簡単な編集で削除できてしまう。また芸術や商業目的で画像を利用する上で美的な問題が生じる。すでに目に見えない透かしを入れる手法もあるが、やはりサイズ変更などの単純な編集で削除できてしまう場合がある。

SynthIDは画質を損なわず、フィルターの追加、色の変更、JPEGをはじめ非可逆圧縮方式でのデータ保存を経ても、透かしを検出可能な状態を維持できるよう設計してある。透かしの挿入と識別に、さまざまな画像セットで訓練した2つの深層学習モデルを使用している。

:::box

:::</description><pubDate>Sat, 02 Sep 2023 06:06:13 +0000</pubDate></item><item><title>米国で「生成AI地ビール」ChatGPTがレシピ作成 Midjourneyがラベルデザイン
</title><link>https://ledge.ai/articles/night_shift_brewing_chatgpt</link><description>:::small
画像出典：Night Shift Brewing
:::

米国のクラフトブルワリーが、AIを活用し新しい地ビールを開発している。ボストンのNight Shift Brewing（ナイトシフト・ブルーイング）は2023年2月「{target=“_blank”}」という地ビールを発表した。

!
:::small
画像出典：Night Shift Brewing
:::
公式サイトによると、ビールのスタイル、完全なレシピ、名前、ラベルはすべてAIがデザインした。対話型AI（チャットボット）「ChatGPT」 と画像生成AI「Midjourney」を組み合わせた。人間は テキストによる指示「プロンプト」や質問を通じて工程を誘導したが、ほとんどの部分でAIが決定を下したという。ビール自体はマンゴーとスイカの弾けるような味わいが楽しめ、生き生きとしたレモンとライムの香りも感じられるという。

またミシガン州デトロイトのAtwater Brewery（アトウォーター・ブルワリー）も2023年1月に「{target=“_blank”}」を発表。さらにニューメキシコ州のRio Bravo Brewing Company（リオ・ブラボー・ブルーイング）は、4月にAIを使用したペールエール「{target=“_blank”}」を市場に投入している。

!
:::small
出典：Atwater Brewery
:::

!
:::small
Rio Bravo Brewing Company
:::

アーティフィシャル・インテリジェンスIPAもAIがデザインしたという銘柄。往年のSF映画「2001年宇宙の旅」に登場するAI「HAL 9000」 のオマージュとして、価格は1パイントあたり2.001ドルに設定した。



アルゴリズムもAI生成を打ち出しており、センテニアルやアマリロなどの昔ながらのホップを使用し、グレープフルーツのオレンジマーマレード風味を感じられる仕上がりになったとのこと。




:::box

:::
:::box

:::

</description><pubDate>Fri, 01 Sep 2023 06:49:40 +0000</pubDate></item><item><title>Googleドキュメント AIが文章を手直しする「Proofread」機能を正式導入</title><link>https://ledge.ai/articles/google_docs_proofread</link><description>米国Googleは、オンライン文章編集アプリケーション「Googleドキュメント」で、書いた文章をAIが読みやすく手直しする「{target=“_blank”}」機能を正式導入する。2023年8月29日に同社のイベント「Google Cloud Next ‘23」で明らかにした。有料のオフィススイート「Google Workspace」で「Duet AI for Workspace Enterprise」アドオンを使用していれば使えるようになる。

Proofread機能は、従来のGoogleドキュメントと同じように綴（つづ）りや文法が適切かどうかを調べてくれるばかりか、新たに文章そのものについて改善案を提示する。文中の単語や言い回しを自動で強調し、それぞれよりよいと考えられる候補を案内して置き換えるかどうかの確認を求める。提案の種類は例えば次のようなものがある。

- Spelling：綴りの間違いを指摘し正しい綴りを示す
- Grammar：文法の間違いを指摘し正しい文法を示す
- Conciseness： 言い回しをより簡潔にする
- Active voice: 言い回しをより読者にとって明確にする
- Wording: 文脈の中で言葉をより躍動的に、またはよりきちんとした使い方にする
- Sentence Split: 文章を読みやすくするために複雑な文を分割する

提案はすべて本文の横の校正欄にまとまるため、共同編集作業中なども邪魔にならないとしている。どの提案を受け入れるかは個別に選べる。ちなみに、この機能はDuet AI for Workspace Enterpriseアドオンを入れれば標準で有効になるが、任意で無効にでき、また設定で表示する提案の種類も絞り込める。

なおすでにProofreadのアルファ版を利用していた場合、継続するためには前述のアドオンを購入する必要がある。

:::box

:::</description><pubDate>Fri, 01 Sep 2023 07:58:29 +0000</pubDate></item><item><title>スペイン 車載AI用に「光通信半導体」の新工場 米中依存から脱却 欧州の自立めざす</title><link>https://ledge.ai/articles/kdpof_fabrication_facility</link><description>:::small
画像出典：写真AC
:::

スペインの光半導体メーカーKDPOFは、車載AIなどに利用できる光通信用半導体の製造施設を建設する予定を{target=“_blank”}した。建設には「欧州共通利益に適合する重要プロジェクト（IPCEI）」から資金提供を受ける。デジタル分野における、欧州地域の自立性の確立を目指して設立した基金だ。

調達した資金を投じてマドリード州トレス・カントスのKDPOF本社近くに光電子機器用のパッケージング工場の建設を進める。さらに同社は新しい光電子パッケージング技術を開発しており、次期車載高速光通信用トランシーバICの製造に使う。AI、センサー、プロセッサーを自動運転車に組み込むために車両内の光ファイバーリンクを使用してすべての部品を堅牢、低コストで相互接続する技術と位置付ける。

「欧州の工場は世界中の半導体の約10パーセントしか製造していないため、我々はアジアと米国に依存している。当社はほかに先駆けてスペインでの自動車用光電子機器の大量生産を進め、依存を軽減してゆく」とCarlos Pardo（カルロス・パルド）共同創設者兼最高経営責任者（CEO）は述べている。

{target=“_blank”}によると、この施設はスペイン初の商用半導体製造工場になるという。

:::box

:::
:::box

:::
</description><pubDate>Thu, 31 Aug 2023 03:11:05 +0000</pubDate></item><item><title>ベトナム学生にサイバーセキュリティ教育を 日本企業での雇用機会も 機械学習のフォアーが現地大と協力</title><link>https://ledge.ai/articles/fore_vietnam_projcet</link><description>ベトナムの大学生にサイバーセキュリティ教育や訓練を実施し、日本企業への雇用機会を作る計画を東京・神田の企業フォアーが2023年8月30日に{target=“_blank”}した。現地のDuy Tan大学（DUT）、人材サービスのSuganuma Groupと同分野の協力に関する覚書を締結している。

フォアーは機械学習などのデータサイエンス事業とサイバーセキュリティ事業を手掛けてきた。今回の覚書に基づきDUTの内部にサイバー攻撃に対する訓練と研究のための共同研究所を設立。DUTの学生向けにサイバーセキュリティに関する教育活動を実施する。

またセキュリティに関する共同研修や研究プログラムに参加する学生を、資金や学習ツールのスポンサーなどさまざまな形で支援する。これに加えサイバーセキュリティ分野の日本企業と学生との連携を強化し、学生の雇用機会を作るという。

DUTは1994年にベトナム中部ダナン市で設立。2015年に私立大学へ移行した。ベトナムおける最初にして最大の私立大学といい、多様で学際的な教育が特徴。情報技術、電気電子の分野で米国の非政府教育認定機関Accreditation Board for Engineering and Technology（ABET）の認定を取得している。英国高等教育情報誌「The Times Higher Education（THE）」の発表する世界大学ランキングでは2023年に上位500位に入った。

:::box

:::</description><pubDate>Thu, 31 Aug 2023 04:33:22 +0000</pubDate></item><item><title>MicrosoftがAIを攻撃する専門組織「AIレッドチーム」ノウハウも公開 他社にも設立呼びかけ</title><link>https://ledge.ai/articles/microsoft_ai_red_team</link><description>:::small
画像はSDXL1.0によりLedge.aiが生成
:::
米国Microsoft は2023年8月7日、公式ブログで「AIレッドチーム」の実践に関する知見を{target=“_blank”}した。自社のAIをあえて攻撃する組織を作り上げ、セキュリティ上の弱点を見つけ出す取り組みだ。

サイバーセキュリティの世界ではシステムの防御役「Blue Team（ブルーチーム）」とは別に攻撃役「Red Team（レッドチーム）」を作り、模擬的に戦わせることで弱点を見つけ、本当の攻撃がある前に対策をとる手法が広がっている。

Microsoftは2018年から「AIレッドチーム」を動かし、自社のAIなどの弱点を探っている。例えば生成AI検索「Bing AI Chat（新しいBing）」の公開前に、Microsoft社内の数十人のセキュリティと責任あるAI（Responsible AI、RAI）の専門家でAIレッドチームを作り、何百時間にもわたって調査をした。このような場合にはBing AI Chatのようなアプリケーションと、背後で動く大規模言語モデル（LLM）「GPT-4」の両方を標的に2段階で取り組む必要があるという。

まずGPT-4に攻撃をしかけることで弱点や限界を見つけることは、GPT-4そのものの将来の改良に役立つだけでなく、どんなアプリに適しているのかを把握する上でも有益だった。Bing AI Chatへの攻撃ではGPT-4を検索に組み込んでも障害が起きないかを確認できた。

今回Microsoftは経験を活かし、AIレッドチームが従来のレッドチームとどこが似ていて、どこが異なるのかをセキュリティ専門家向けに説明。より多くの組織がAIレッドチームを作り、さらに既存のレッドチームをよりうまく活用できるようにする参考として、5つの知見を紹介している。

!

**１．AIレッドチームの役目はより広範囲:**
AIレッドチームは従来のレッドチームと異なりセキュリティ脆弱性だけでなく、RAI問題を取り扱わねばならない。AIが示すさまざまな固定観念など公平性の問題や、暴力の賛美をはじめとする有害な情報についても調査する必要がある。

**２．AIレッドチームは悪意の人物だけでなく善意の人物が引き起こす問題にも注目する:**
例えば新しいBingでは、悪意の人物がセキュリティに焦点を当てた技術や不正なプログラムでAIを破壊する可能性だけでなく、いかに一般人がAIとの対話で有害な情報を生成するかにも注目せねばならなかった。この点が悪意の人物の行動のみに注目する従来のレッドチームとは異なる。AIレッドチームでは、より広範なペルソナ（想定する人物像）を対象にする。

**３．AIは絶えず進化している:**
AI常に変化する。LLMの場合、開発者はフィードバックに基づいてメタプロンプト（機械学習モデルへの基礎となる命令）を変更することがある。伝統的なソフトウェア・システムに比べAIは急速に変化するため、AIレッドチームは攻撃を繰り返し仕掛け、体系的で自動化した測定システムを確立し、時間をかけて監視することが重要。

**４．生成AIに対しては複数回の試行が必要:**
生成AIのふるまいは確率によって決まり、同じ入力を複数回実行すると、異なる出力が得られる可能性がある。AIレッドチームも同じ操作で複数回の攻撃を実施する必要がある。初回の攻撃が失敗しても、次回以降の同じ攻撃が成功してしまう場合を考慮せねばならない。攻撃に対する結果が決定的になる従来のレッドチームとの違いだ。

**５．AIの問題を緩和するには多層防御が必要:**
AIに起こる問題を緩和するためには、単一の方法ではなく複数の手段を組み合わせる必要がある。潜在的に有害な情報に目印をつけておくためにAIの「分類器（Classifier）」を使用することや、人間との対話が危険な方向へずれてゆかないよう「メタプロンプト」を使用して行動を誘導することを含む。多層防御が重要という点については、既存のレッドチームと似通う。

:::box

:::
:::box

:::
</description><pubDate>Wed, 30 Aug 2023 10:54:10 +0000</pubDate></item><item><title>渦中のAP通信「生成AIを記事執筆に使用しない」ガイドライン公表 ただしOpenAIとは協力</title><link>https://ledge.ai/articles/ap_standards_around_generative_ai</link><description>米国AP通信は2023年8月16日、{target=“_blank”}を公表した。生成AIを記事執筆には使用しないとあらためて明言した。同社は7月に対話型AI（チャットボット）「ChatGPT」を開発するOpenAIとライセンス契約を結んで注目を浴びたところ。

APは公式ブログで「我々は、報道における正確性、公平性、迅速性を重視しており、AIを注意深く使用することでその価値を向上でき、働き方にも貢献できると考えている」と前置きした。

そのうえで「AP通信のジャーナリストの中心的な役割は変わらず、AIがその代替になり得るとは考えない。ジャーナリストは、共有する情報の正確性と公平性について責任を負わねばならない」と述べた。

APとOpenAIが結んだライセンス契約では、OpenAIがAPのテキストアーカイブの一部に対する使用許諾を得て、APはOpenAIの技術と製品の専門知識を活用できる。しかし、APは公開する記事などの作成に生成AIは使用しない。

またAIが生成する内容は未確認の情報源として扱うとし、公開時にはAPの編集上の判断と情報源に関する基準を適用する。AIで写真、動画、音声の要素を改変しないこと、AIが虚偽を描写したと疑いうるあるいはそう証明済みのA画像も配信しないことを明確にした。

ただし、ただしAIが生成したイラストや芸術作品が記事の主題になる場合、その旨を明記している限り使用できるとした。

APは社内の記者たちに、機密情報や機微情報をAIに入力しないよう強く求めている。また、外部から受け取った素材についてはAIが生成したものをを含んでいないか確認するよう注意を促しているという。素材の出典を確認したり、逆画像検索を行ったり、信頼できる報道機関で類似の素材を使用していないか確認を怠らないこと、また資料の信憑性に疑念がある場合は使用しないよう求めているとのこと。

:::box

:::
:::box

:::</description><pubDate>Wed, 30 Aug 2023 02:16:51 +0000</pubDate></item><item><title>Naver 独自の大規模言語モデル「Hyper CLOVA X」発表 チャットボットやAI検索も</title><link>https://ledge.ai/articles/hyper_clova_x</link><description>韓国Naverは2023年8月24日、ソウル市で生成AI技術と事業戦略を中心としたカンファレンス「DAN 23」を開催した。のチェ・スヨン代表が、大規模言語モデル（LLM）「{target=“_blank”} X」、対話型AI（チャットボット）「{target=“_blank”}」、生成AI検索「CUE」の3つの{target=“_blank”}した。

このうちCLOVA Xは創作、要約、推論、翻訳、コーディングなどをベースに多様な回答を提供できる対話型AIで、質問と回答を連続して続けられる。Naver内外の様々なサービスをアプリケーション・プログラミング・インターフェイス（API）でつなぐ「スキル」機能を導入し、LLMだけでは限界がある回答を補完してゆくことが可能。NaverはもちろんLINEやFacebookのアカウントで利用登録を受け付けている。

なおNaverは60万台以上のサーバーを収容でき、単一企業としてはアジア最大規模をうたうデータセンターを11月に世宗（セジョン）特別自治市で開設予定だとも明らかにした。

:::box

:::
:::box

:::</description><pubDate>Wed, 30 Aug 2023 08:46:05 +0000</pubDate></item></channel></rss>